# Sportsbook Data Platform

End-to-end data platform project simulating a real-world Sportsbook analytics environment.  
The project covers ingestion, transformation, orchestration, data quality and visualization using modern open-source tools.

This repository was built as a **production-oriented case study**, not a tutorial.

---

## ğŸ—ï¸ Architecture Overview

**Flow:**

Raw CSV files  
â†’ Python ETL  
â†’ PostgreSQL (Raw Layer)  
â†’ dbt (Staging & Star Schema)  
â†’ Data Quality (Great Expectations)  
â†’ Orchestration (Airflow)  
â†’ Visualization (Metabase)

**Layers:**
- **Raw**: source-aligned ingestion
- **Staging**: cleaned and standardized models
- **Mart**: star schema (facts & dimensions)

---

## ğŸ§° Tech Stack

- **Python 3.10**
- **PostgreSQL 15**
- **Apache Airflow 2.8**
- **dbt Core 1.7 (Postgres adapter)**
- **Great Expectations**
- **Metabase**
- **Docker & Docker Compose**

---

## ğŸ“‚ Project Structure

â”œâ”€â”€ airflow/
â”‚ â”œâ”€â”€ dags/
â”‚ â”‚ â””â”€â”€ sportsbook_pipeline.py
â”‚ â””â”€â”€ Dockerfile
â”œâ”€â”€ dbt/
â”‚ â”œâ”€â”€ models/
â”‚ â”‚ â”œâ”€â”€ staging/
â”‚ â”‚ â”œâ”€â”€ dimensions/
â”‚ â”‚ â””â”€â”€ facts/
â”‚ â”œâ”€â”€ dbt_project.yml
â”‚ â””â”€â”€ profiles.yml
â”œâ”€â”€ etl/
â”‚ â””â”€â”€ load_raw.py
â”œâ”€â”€ great_expectations/
â”‚ â”œâ”€â”€ checkpoints/
â”‚ â””â”€â”€ great_expectations.yml
â”œâ”€â”€ postgres/
â”‚ â””â”€â”€ init.sql
â”œâ”€â”€ data/
â”‚ â””â”€â”€ raw/
â”‚ â””â”€â”€ *.csv
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md



---

## â­ Data Modeling

The data warehouse follows a **Star Schema** design.

### Fact Tables
- `fact_bets`
  - turnover
  - winnings
  - gross revenue
  - customer reference

### Dimension Tables
- `dim_customer`
- `dim_event`
- `dim_sport` (extensible)

This design enables:
- fast analytical queries
- BI-friendly consumption
- clear business semantics

---

## â±ï¸ Orchestration (Airflow)

The pipeline is orchestrated using **Apache Airflow**.

### DAG: `sportsbook_pipeline`

**Steps:**
1. Python ETL â†’ load CSVs into `raw` schema
2. `dbt run` â†’ build staging & mart models
3. `dbt test` â†’ validate transformations
4. Great Expectations â†’ data quality checks

The DAG is **trigger-based** (no schedule by default).

---

## ğŸ§ª Data Quality

**Great Expectations** is used to validate critical tables.

Examples:
- No null primary keys
- Revenue values â‰¥ 0
- Referential integrity between facts and dimensions

Quality checks run automatically after dbt transformations.

---

## ğŸ“Š Visualization

**Metabase** connects directly to PostgreSQL and exposes:

- Star schema tables
- Business metrics
- Dashboards (bets, revenue, customers)

Metabase runs fully containerized.

---

## ğŸš€ How to Run Locally (Ubuntu)

### Prerequisites
- Docker
- Docker Compose

### 1ï¸âƒ£ Clone the repository


git clone https://github.com/flipg1995/sportsbook-data-platform.git
cd sportsbook-data-platform

### 2ï¸âƒ£ Start the platform

docker compose up --build

Airflow	http://localhost:8080
Metabase http://localhost:3000
PostgreSQL http://localhost:5432

### â–¶ï¸ Run the Pipeline

Open Airflow UI

Enable DAG sportsbook_pipeline

Trigger the DAG manually

If all tasks succeed, the data warehouse is ready.

### ğŸ” Database Connection (Metabase)

Host: postgres

Database: sportsbook_dw

User: dw_user

Password: dw_pass

Port: 5432